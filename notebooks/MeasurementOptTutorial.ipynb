{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6FhcTY09dK4o",
        "outputId": "bd0506f7-ea12-40e5-d9d4-ee901ea759b3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting numpy==1.26.4\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.0/61.0 kB\u001b[0m \u001b[31m649.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.26.4\n",
            "Collecting jax==0.4.28\n",
            "  Downloading jax-0.4.28-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting jaxlib==0.4.28\n",
            "  Downloading jaxlib-0.4.28-cp311-cp311-manylinux2014_x86_64.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: ml-dtypes>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.28) (0.4.1)\n",
            "Requirement already satisfied: numpy>=1.22 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.28) (1.26.4)\n",
            "Requirement already satisfied: opt-einsum in /usr/local/lib/python3.11/dist-packages (from jax==0.4.28) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.9 in /usr/local/lib/python3.11/dist-packages (from jax==0.4.28) (1.15.2)\n",
            "Downloading jax-0.4.28-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m25.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jaxlib-0.4.28-cp311-cp311-manylinux2014_x86_64.whl (77.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.5/77.5 MB\u001b[0m \u001b[31m8.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: jaxlib, jax\n",
            "  Attempting uninstall: jaxlib\n",
            "    Found existing installation: jaxlib 0.5.1\n",
            "    Uninstalling jaxlib-0.5.1:\n",
            "      Successfully uninstalled jaxlib-0.5.1\n",
            "  Attempting uninstall: jax\n",
            "    Found existing installation: jax 0.5.2\n",
            "    Uninstalling jax-0.5.2:\n",
            "      Successfully uninstalled jax-0.5.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "orbax-checkpoint 0.11.13 requires jax>=0.5.0, but you have jax 0.4.28 which is incompatible.\n",
            "flax 0.10.6 requires jax>=0.5.1, but you have jax 0.4.28 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed jax-0.4.28 jaxlib-0.4.28\n"
          ]
        }
      ],
      "source": [
        "#This cell you will need to run it twice (First time will crash)\n",
        "!pip install numpy==1.26.4\n",
        "!pip install jax==0.4.28 jaxlib==0.4.28\n",
        "import numpy as np\n",
        "print(np.__version__)\n",
        "if int(np.__version__[0]) > 1:\n",
        "  import os\n",
        "  os.kill(os.getpid(), 9)\n",
        "\n",
        "!pip install typing-extensions --upgrade\n",
        "!pip install pennylane update\n",
        "!pip install tqdm\n",
        "!pip install git+https://github.com/quantumlib/OpenFermion.git@master#egg=openfermion\n",
        "!pip install --upgrade git+https://github.com/aspuru-guzik-group/tequila.git@devel\n",
        "!pip install PySCF"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pennylane as qml\n",
        "import pennylane.numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import time\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from tqdm import tqdm, trange\n",
        "from openfermion.transforms import *\n",
        "from openfermion.ops import FermionOperator, QubitOperator, BinaryCode, BinaryPolynomial\n",
        "import tequila as tq\n",
        "from tequila.hamiltonian import QubitHamiltonian, paulis\n",
        "from tequila.grouping.binary_rep import BinaryHamiltonian\n",
        "from tequila.grouping import *\n",
        "import networkx as nx\n",
        "from networkx import Graph\n",
        "import numpy as np\n",
        "from openfermion.linalg import get_ground_state, get_sparse_operator, variance\n",
        "from openfermion.utils import count_qubits\n",
        "import math\n",
        "from jax import random\n",
        "from jax import numpy as jnp\n",
        "from collections import defaultdict\n"
      ],
      "metadata": {
        "id": "HUpJDrKQeFfG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def H2():\n",
        "    '''\n",
        "    H2 hamiltonian for minimal basis.\n",
        "    '''\n",
        "    trafo = \"JordanWigner\"\n",
        "    mol = tq.chemistry.Molecule(\n",
        "                            geometry=\"H 0.0 0.0 0.0 \\n H 0.0 0.0 1.0\",\n",
        "                            basis_set=\"sto3g\",\n",
        "                            transformation=trafo,\n",
        "                            backend='pyscf'\n",
        "                                 )\n",
        "    H = mol.make_hamiltonian()\n",
        "    Hq = H.to_openfermion()\n",
        "    Hferm = reverse_jordan_wigner(Hq)\n",
        "    n_paulis = len(Hq.terms) - 1 #Minus 1 since it always contain a constant term that we don't need to measure.\n",
        "    return mol, H, Hferm, n_paulis, Hq"
      ],
      "metadata": {
        "id": "49GW_LmPZFq-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Measurement problem and measurement optimization"
      ],
      "metadata": {
        "id": "Wc4O1pOI3zc1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "We will need **OpenFermion** and more importantly **Tequila** to get through this."
      ],
      "metadata": {
        "id": "kmFjxhg_7ERK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mol, H, Hferm, n_paulis, Hq = H2()\n",
        "\n",
        "H = qml.qchem.import_operator(Hq)\n",
        "H_coeffs, H_ops = H.terms()\n",
        "print(H)\n",
        "m = qml.resource.estimate_shots(H_coeffs)\n",
        "print(f'Estimated Shots w/o grouping : {m}')"
      ],
      "metadata": {
        "id": "nFMP9yQoymQJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ops, coeffs = qml.pauli.group_observables(H_ops, H_coeffs, 'qwc', 'rlf')\n",
        "c_qwc = [np.array(c) for c in coeffs] # cast as numpy array\n",
        "m = qml.resource.estimate_shots(c_qwc)\n",
        "print(f'Shots grouped by PennyLane RLF QWC: {m}')#This is just an estimate!!"
      ],
      "metadata": {
        "id": "pdI9Z47p45kX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "ops, coeffs = qml.pauli.group_observables(H_ops, H_coeffs, 'commuting', 'rlf')\n",
        "c_fc = [np.array(c) for c in coeffs] # cast as numpy array\n",
        "m = qml.resource.estimate_shots(c_fc)\n",
        "print(f'Shots grouped by PennyLane RLF FC: {m}')#This is just an estimate!!"
      ],
      "metadata": {
        "id": "GEMdibBy8jcx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "error = np.array([0.02, 0.015, 0.01, 0.005, 0.001])\n",
        "m_qwc = [qml.resource.estimate_shots(c_qwc, error=er) for er in error]\n",
        "m_fc = [qml.resource.estimate_shots(c_fc, error=er) for er in error]\n",
        "\n",
        "e_ = np.linspace(error[0], error[-1], num=50)\n",
        "m_ = 1.0 / np.linspace(error[0], error[-1], num=50)**2\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 6))\n",
        "ax.plot(error, m_qwc, 'v', color='red', label='QWC')\n",
        "ax.plot(error, m_fc, 'v', color='blue', label='FC')\n",
        "\n",
        "ax.plot(e_, m_, ':', markerfacecolor='none', color='olive', label='$ 1/\\epsilon^2 $')\n",
        "\n",
        "ax.set_ylabel('shots')\n",
        "ax.set_xlabel('error [Ha]')\n",
        "ax.set_yscale('log')\n",
        "ax.tick_params(axis='x', labelrotation = 90)\n",
        "ax.legend()\n",
        "fig.tight_layout()"
      ],
      "metadata": {
        "id": "11U-pZOb5R05"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "There are other approaches to grouping.\n",
        "\n",
        "We will explore a generative model called **[GFlowNets](https://arxiv.org/abs/2111.09266)** and use **[Tequila](https://github.com/tequilahub/tequila)** which has better implementations which require variance estimators."
      ],
      "metadata": {
        "id": "mK5agcdEBuYz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from openfermion.transforms import *\n",
        "from openfermion.ops import FermionOperator, QubitOperator, BinaryCode, BinaryPolynomial\n",
        "import tequila as tq\n",
        "from tequila.hamiltonian import QubitHamiltonian, paulis\n",
        "from tequila.grouping.binary_rep import BinaryHamiltonian\n",
        "from tequila.grouping import *\n",
        "import networkx as nx\n",
        "from networkx import Graph\n",
        "import numpy as np\n",
        "from openfermion.linalg import get_ground_state, get_sparse_operator, variance\n",
        "from openfermion.utils import count_qubits\n",
        "import math\n",
        "from jax import random\n",
        "from jax import numpy as jnp\n",
        "from collections import defaultdict"
      ],
      "metadata": {
        "id": "rpR6fVe5CeNm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions to generate the Hamiltonian and Complementary Graphs"
      ],
      "metadata": {
        "id": "dHgmyRc5i0N2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def get_terms(bin_H):\n",
        "    \"\"\" Gets the terms from a Binary Hamiltonian excluding the constant term\"\"\"\n",
        "    terms=[]\n",
        "    for i in range(1,len(bin_H.binary_terms)):\n",
        "        terms.append(bin_H.binary_terms[i])\n",
        "    return terms\n",
        "\n",
        "def FC_CompMatrix(terms):\n",
        "    \"\"\" Generates the Fully commuting complementary matrix\"\"\"\n",
        "    FC_CompMatrix=[]\n",
        "    rows=len(terms)\n",
        "    cols=len(terms)\n",
        "    FC_CompMatrix = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            if terms[i].commute(terms[j]):\n",
        "                FC_CompMatrix[i][j]=1\n",
        "                FC_CompMatrix[j][j]=1 #Avoids self loops\n",
        "            else:\n",
        "                FC_CompMatrix[i][j]=0\n",
        "    return FC_CompMatrix\n",
        "\n",
        "def QWC_CompMatrix(terms):\n",
        "    \"\"\" Generates the Qubit-wise commuting complementary matrix\"\"\"\n",
        "    QWC_CompMatrix=[]\n",
        "    rows=len(terms)\n",
        "    cols=len(terms)\n",
        "    QWC_CompMatrix = [[0 for _ in range(cols)] for _ in range(rows)]\n",
        "    for i in range(rows):\n",
        "        for j in range(cols):\n",
        "            if terms[i].qubit_wise_commute(terms[j]):\n",
        "                QWC_CompMatrix[i][j]=1\n",
        "                QWC_CompMatrix[j][j]=1 #Avoids self loops\n",
        "            else:\n",
        "                QWC_CompMatrix[i][j]=0\n",
        "    return QWC_CompMatrix\n",
        "\n",
        "def obj_to_comp_graph(terms, matrix) -> Graph:\n",
        "        \"\"\"Convert terms from a compCommMatrix to a Graph\"\"\"\n",
        "        g = nx.Graph()\n",
        "        matrix = matrix  # Make a copy\n",
        "        terms = terms\n",
        "        for a in range(len(terms)):\n",
        "            g.add_node(\n",
        "                a,\n",
        "                v=terms[a].to_pauli_strings(),\n",
        "            )\n",
        "        for i in range(len(terms)):\n",
        "            for j in range(i,len(terms)):\n",
        "                if matrix[i][j] == 0:\n",
        "                    g.add_edge(\n",
        "                    i,\n",
        "                    j,\n",
        "                )\n",
        "        return g"
      ],
      "metadata": {
        "id": "HX9Gig9G6tDN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Functions for getting measurements from Graphs"
      ],
      "metadata": {
        "id": "K_RJMHWcDO0S"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def extract_hamiltonian_by_color(graph):\n",
        "    \"\"\"\n",
        "    Extracts the hamiltonian groups based on the coloring of the graph.\n",
        "    \"\"\"\n",
        "    hamiltonian_terms = defaultdict(list)\n",
        "\n",
        "    for node, data in graph.nodes(data=True):\n",
        "        color = data['color']\n",
        "        pauli_string = data['v']\n",
        "        #print(type(pauli_string))\n",
        "        hamiltonian_terms[color].append(pauli_string)\n",
        "\n",
        "    return dict(hamiltonian_terms)\n",
        "\n",
        "def generate_groups(hamiltonian_by_color):\n",
        "    \"\"\"\n",
        "    This generates the groups from a colored Hamiltonian in an openfermion format\n",
        "    \"\"\"\n",
        "    groups = []\n",
        "\n",
        "    for color, pauli_strings in hamiltonian_by_color.items():\n",
        "        # Create a QubitHamiltonian from the PauliString list\n",
        "        qubit_hamiltonian = QubitHamiltonian.from_paulistrings(pauli_strings)\n",
        "        # Convert to OpenFermion QubitOperator\n",
        "        openfermion_operator = qubit_hamiltonian.to_openfermion()\n",
        "        # Append to the groups list\n",
        "        groups.append(openfermion_operator)\n",
        "\n",
        "    return groups\n",
        "\n",
        "def convert_operators(groups):\n",
        "    \"\"\"In the current implementation we need to convert operator to Pennylane\n",
        "    to estimate the total number of shots. \"\"\"\n",
        "    imported_operators = []\n",
        "\n",
        "    for hamiltonian in groups:\n",
        "        # Import the QubitOperator into PennyLane format\n",
        "        operator = qml.import_operator(hamiltonian, format='openfermion')\n",
        "        imported_operators.append(operator)\n",
        "\n",
        "    return imported_operators\n",
        "\n",
        "def estimate_shots(imported_operators):\n",
        "    \"\"\"\n",
        "    Estimate the number of shots required based on a list of imported PennyLane operators.\n",
        "\n",
        "    Parameters:\n",
        "    - imported_operators (list of qml.PauliSum): List of PennyLane operators.\n",
        "\n",
        "    Returns:\n",
        "    - list of float: Estimated number of shots for each operator.\n",
        "    \"\"\"\n",
        "    # Initialize list to hold estimated shots for each operator\n",
        "    estimated_shots_list = []\n",
        "    all_coeffs = []\n",
        "    # Iterate over each imported operator\n",
        "    for operator in imported_operators:\n",
        "        # Extract coefficients and observables from each operator\n",
        "        coeffs, ops = operator.terms()\n",
        "\n",
        "        # Convert coefficients to numpy arrays for compatibility with PennyLane\n",
        "        all_coeffs.append(np.array(coeffs))\n",
        "\n",
        "    #print(all_coeffs)\n",
        "    # Estimate shots for the grouping\n",
        "    estimated_shots = qml.resource.estimate_shots(all_coeffs)\n",
        "\n",
        "    return estimated_shots\n",
        "\n",
        "def shots_estimator(graph):\n",
        "    \"\"\"\n",
        "    Full function that estimates the required number of shots to achieve chemical accuracy.\n",
        "    Classifies the H by color, generates groups, translates to pennylane and estimates shots\n",
        "    \"\"\"\n",
        "    h_by_color = extract_hamiltonian_by_color(graph)\n",
        "    groups = generate_groups(h_by_color)\n",
        "    imported_operators = convert_operators(groups)\n",
        "    estimated_shots = estimate_shots(imported_operators)\n",
        "    return estimated_shots"
      ],
      "metadata": {
        "id": "-vhF9_RijHPp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Grouping with a generative model. GFlowNets"
      ],
      "metadata": {
        "id": "8HAwsnCZjMpe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "mol, H, Hferm, n_paulis, Hq = H2()\n",
        "print(\"Number of Pauli products to measure: {}\".format(n_paulis))\n",
        "\n",
        "sparse_hamiltonian = get_sparse_operator(Hq)\n",
        "energy, fci_wfn = get_ground_state(sparse_hamiltonian)\n",
        "print(\"Energy={}\".format(energy))\n",
        "n_q = count_qubits(Hq)\n",
        "print(\"Number of Qubits={}\".format(n_q))\n",
        "#Get list of Hamiltonian terms and generate complementary graph\n",
        "binary_H = BinaryHamiltonian.init_from_qubit_hamiltonian(H)\n",
        "terms=get_terms(binary_H)\n",
        "#CompMatrix=FC_CompMatrix(terms)\n",
        "CompMatrix=QWC_CompMatrix(terms)\n",
        "Gc=obj_to_comp_graph(terms, CompMatrix)\n",
        "n_terms=nx.number_of_nodes(Gc)\n",
        "print(\"Number of terms in the graph: {}\".format(n_terms))\n"
      ],
      "metadata": {
        "id": "Awj-6nP_DFKf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pos = nx.circular_layout(Gc)\n",
        "options = {\n",
        "    \"pos\": pos,\n",
        "    \"node_size\": 1200,\n",
        "    \"edge_color\": \"gray\",\n",
        "    \"alpha\": 1,\n",
        "    \"width\": 4,\n",
        "    \"labels\": {n: n for n in Gc}\n",
        "}\n",
        "plt.figure(1,figsize=(7,7))\n",
        "nx.draw_networkx(Gc,**options)"
      ],
      "metadata": {
        "id": "tlppy809hj99"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Grouping with GFlowNets"
      ],
      "metadata": {
        "id": "l98bIwKi1ADa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "For GFlowNets, we will employ a **Reward function** ($R(x)$) and require the definition for a model.\n",
        "\n",
        "Here we will use the **Trajectory Balance** condition."
      ],
      "metadata": {
        "id": "ObA1gMdW6Tjd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from tqdm import tqdm, trange\n",
        "from torch.distributions.categorical import Categorical\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import random"
      ],
      "metadata": {
        "id": "_qIzT5R566wi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Define the Model, Masking functions and the loss function."
      ],
      "metadata": {
        "id": "W0AMFVK27Wwb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TBModel(nn.Module):\n",
        "  def __init__(self, num_hid, n_terms):\n",
        "    super().__init__()\n",
        "    self.mlp = nn.Sequential(\n",
        "        nn.Linear(n_terms, num_hid),  # layers (Number of Paulis) input features.\n",
        "        nn.LeakyReLU(),\n",
        "        nn.Linear(num_hid, 2*n_terms),  # double outputs: 1/2 for P_F and 1/2 for P_B.\n",
        "    )\n",
        "    self.logZ = nn.Parameter(torch.ones(1))  # log Z is just a single number.\n",
        "\n",
        "  def forward(self, x, n_terms):\n",
        "    logits = self.mlp(x)\n",
        "    # Slice the logits into forward and backward policies.\n",
        "    P_F = logits[..., :n_terms]\n",
        "    P_B = logits[..., n_terms:]\n",
        "\n",
        "    return P_F, P_B\n",
        "\n",
        "def calculate_forward_mask_from_state(state, t, lower_bound):\n",
        "    \"\"\"We want to mask the sampling to avoid any potential loss of time while training.\n",
        "    In order to do so, we will have an upper bound on the number of colors used. Additionally,\n",
        "    we can make the probability of using the same color in 2 neighbors = 0 to ensure validity.\n",
        "    \"\"\"\n",
        "    layers=nx.number_of_nodes(state)\n",
        "    mask = np.ones(layers)  # Allowed actions represented as 1, disallowed actions as 0.\n",
        "    mask[lower_bound+1:] = 0\n",
        "    neighbors = list(state.neighbors(t))\n",
        "    neighbor_colors = [state.nodes[n]['color'] for n in neighbors]\n",
        "    # Update the mask\n",
        "    for color in neighbor_colors:\n",
        "        mask[color] = 0\n",
        "    return torch.Tensor(mask).bool()\n",
        "\n",
        "def calculate_backward_mask_from_state(state,t,lower_bound):\n",
        "    \"\"\"Here, we mask backward actions to only select parent nodes.\"\"\"\n",
        "    # This mask should be 1 for any action that could have led to the current state,\n",
        "    # otherwise it should be zero.\n",
        "    layers=nx.number_of_nodes(state)\n",
        "    mask = np.ones(layers)  # Allowed actions represented as 1, disallowed actions as 0.\n",
        "    mask[lower_bound+1:] = 0\n",
        "    neighbors = list(state.neighbors(t))\n",
        "    neighbor_colors = [state.nodes[n]['color'] for n in neighbors]\n",
        "    # Update the mask\n",
        "    for color in neighbor_colors:\n",
        "        mask[color] = 1\n",
        "    return torch.Tensor(mask).bool()\n",
        "\n",
        "def trajectory_balance_loss(logZ, log_P_F, log_P_B, reward):\n",
        "    \"\"\"Trajectory balance objective converted into mean squared error loss.\"\"\"\n",
        "    reward=torch.tensor(reward).float()\n",
        "    return (logZ + log_P_F - torch.log(torch.clamp(reward, min=1e-30)) - log_P_B).pow(2)"
      ],
      "metadata": {
        "id": "3dyTTtrm7Tlp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Translate Graph to something trainable and with a reward."
      ],
      "metadata": {
        "id": "-9KC4PG08xgw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def graph_hash(graph):\n",
        "    \"\"\"Returns a binary hash for each submitted graph.\"\"\"\n",
        "    if not nx.get_node_attributes(graph, \"color\"):\n",
        "        # Assign a default color (-1) to all nodes\n",
        "        nx.set_node_attributes(graph, -1, \"color\")\n",
        "\n",
        "    colors_dict = nx.get_node_attributes(graph, \"color\")\n",
        "    FEATURE_KEYS = list(colors_dict.values())\n",
        "\n",
        "    return tuple(FEATURE_KEYS)\n",
        "\n",
        "def graph_to_tensor(graph, verbose=False):\n",
        "  \"\"\"Encodes a graph as a binary tensor (converted to float32).\"\"\"\n",
        "  if verbose:\n",
        "      print(\"graph={}, hash={}, tensor={}\".format(\n",
        "          graph,\n",
        "          graph_hash(graph),\n",
        "          torch.tensor(graph_hash(graph)).float(),\n",
        "          )\n",
        "      )\n",
        "  return torch.tensor(graph_hash(graph)).float()\n",
        "\n",
        "def set_seed(seed):\n",
        "    torch.manual_seed(seed)\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "\n",
        "def is_not_valid(graph):\n",
        "    \"\"\"Checks whether the graph coloring is valid or not.\n",
        "    Returns True if not valid, False if valid.\"\"\"\n",
        "    colors_dict = nx.get_node_attributes(graph, \"color\")\n",
        "    colors_list = list(colors_dict.values())\n",
        "\n",
        "    for i in range(nx.number_of_nodes(graph)):\n",
        "        node_color = colors_list[i]\n",
        "        # Check if any neighbor has the same color as the node\n",
        "        same_color = any(node_color == colors_list[n] for n in graph.neighbors(i))\n",
        "\n",
        "        # If any neighbor has the same color, the graph coloring is not valid\n",
        "        if same_color:\n",
        "            #print(f\"Node {i} has the same color as at least one of its neighbors.\")\n",
        "            return True\n",
        "\n",
        "    # If no conflicts were found, the graph coloring is valid\n",
        "    #print(\"Graph coloring is valid. No neighboring nodes share the same color.\")\n",
        "    return False\n",
        "\n",
        "def max_color(graph):\n",
        "    colors_dict = nx.get_node_attributes(graph, \"color\")\n",
        "    return len(set(colors_dict.values()))\n",
        "\n",
        "def color_reward(graph):\n",
        "  \"\"\"Reward is based on the number of colors we have. The lower the better. Invalid configs give 0\"\"\"\n",
        "  if is_not_valid(graph):\n",
        "    return 0\n",
        "  else:\n",
        "    for i in range(nx.number_of_nodes(graph)+1):\n",
        "        if max_color(graph) == i:\n",
        "            return nx.number_of_nodes(graph)-i #Max number of colors - Actual max color of the graph.\n",
        "\n",
        "def vqe_reward(graph):\n",
        "    \"\"\"Reward is based on the number of colors we have. The lower cliques the better.\n",
        "    Invalid configs give 0. Additionally, employs 10^6/Nshots to achieve chemical accuracy\n",
        "    as reward function. The lower number of shots, the better.\"\"\"\n",
        "    if is_not_valid(graph):\n",
        "        return 0\n",
        "    else:\n",
        "        reward=color_reward(graph) + 10**6/shots_estimator(graph)\n",
        "\n",
        "    return reward\n"
      ],
      "metadata": {
        "id": "FkfbyU2N84rQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Training loop."
      ],
      "metadata": {
        "id": "Yg3sBzx86zNb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def TB_training(graph, n_terms, n_hid_units, n_episodes, learning_rate, update_freq, seed, n_q):\n",
        "\n",
        "    set_seed(seed)\n",
        "\n",
        "    # Instantiate model and optimizer\n",
        "    model = TBModel(n_hid_units,n_terms)\n",
        "    opt = torch.optim.Adam(model.parameters(),  learning_rate)\n",
        "\n",
        "    # Accumulate losses here and take a\n",
        "    # gradient step every `update_freq` episode (at the end of each trajectory).\n",
        "    losses, sampled_graphs, logZs = [], [], []\n",
        "    minibatch_loss = 0\n",
        "    # Determine upper limit\n",
        "    color_map = nx.coloring.greedy_color(graph, strategy=\"largest_first\")\n",
        "    bound=max(color_map.values())+1\n",
        "\n",
        "    tbar = trange(n_episodes, desc=\"Training iter\")\n",
        "    for episode in tbar:\n",
        "        state = graph  # Each episode starts with the initially colored graph\n",
        "        P_F_s, P_B_s = model(graph_to_tensor(state),n_terms)  # Forward and backward policy\n",
        "        total_log_P_F, total_log_P_B = 0, 0\n",
        "\n",
        "        for t in range(nx.number_of_nodes(state)):  # All trajectories as length the number of nodes\n",
        "\n",
        "            #Mask calculator\n",
        "            new_state = state.copy()\n",
        "            mask = calculate_forward_mask_from_state(new_state, t, bound)\n",
        "            P_F_s = torch.where(mask, P_F_s, -100)  # Removes invalid forward actions.\n",
        "            # Sample the action and compute the new state.\n",
        "            # Here P_F is logits, so we use Categorical to compute a softmax.\n",
        "            categorical = Categorical(logits=P_F_s)\n",
        "            action = categorical.sample()\n",
        "            #print('Action {}'.format(action))\n",
        "            new_state.nodes[t]['color'] = action.item()\n",
        "            total_log_P_F += categorical.log_prob(action)  # Accumulate the log_P_F sum.\n",
        "\n",
        "            #If a trajectory is complete. in TB we don't need to calculate parents.\n",
        "            if t == nx.number_of_nodes(state)-1:  # End of trajectory.\n",
        "            # We calculate the reward\n",
        "                reward = vqe_reward(new_state)\n",
        "\n",
        "            # We recompute P_F and P_B for new_state.\n",
        "            P_F_s, P_B_s = model(graph_to_tensor(new_state),n_terms)\n",
        "            mask = calculate_backward_mask_from_state(new_state, t, bound)\n",
        "            P_B_s = torch.where(mask, P_B_s, -100)  # Removes invalid backward actions.\n",
        "\n",
        "            # Accumulate P_B, going backwards from `new_state`.\n",
        "            total_log_P_B += Categorical(logits=P_B_s).log_prob(action)\n",
        "\n",
        "            state = new_state  # Continue iterating.\n",
        "\n",
        "        # We're done with the trajectory, let's compute its loss. Since the reward\n",
        "        # can sometimes be zero, instead of log(0) we'll clip the log-reward to -20.\n",
        "        minibatch_loss += trajectory_balance_loss(\n",
        "            model.logZ,\n",
        "            total_log_P_F,\n",
        "            total_log_P_B,\n",
        "            reward,\n",
        "        )\n",
        "\n",
        "    # We're done with the episode, add the graph to the list, and if we are at an\n",
        "    # update episode, take a gradient step.\n",
        "        sampled_graphs.append(state)\n",
        "        if episode % update_freq == 0:\n",
        "            losses.append(minibatch_loss.item())\n",
        "            logZs.append(model.logZ.item())\n",
        "            minibatch_loss.backward()\n",
        "            opt.step()\n",
        "            opt.zero_grad()\n",
        "            minibatch_loss = 0\n",
        "            # In case you want to save info from the model. You'll need to pass also fig_name to the function.\n",
        "            # torch.save({\n",
        "            # 'epoch': episode,\n",
        "            # 'model_state_dict': model.state_dict(),\n",
        "            # 'optimizer_state_dict': opt.state_dict(),\n",
        "            # 'loss': losses,\n",
        "            # }, fig_name + \"_pureTBmodel.pth\")\n",
        "\n",
        "    return sampled_graphs, losses"
      ],
      "metadata": {
        "id": "bWWgkTST6mZm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Parameters for the training"
      ],
      "metadata": {
        "id": "4DzgUi5F-k4D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_hid_units = 512\n",
        "n_episodes = 5000\n",
        "learning_rate = 5e-3 #1e-4 5e-3  Try different values to see what happens to loss, valid faces and best graphs\n",
        "update_freq = 10\n",
        "seed = 99\n",
        "fig_name = \"Test\"\n",
        "CompMatrix=QWC_CompMatrix(terms)\n",
        "Gc=obj_to_comp_graph(terms, CompMatrix)\n",
        "\n",
        "print(\"For all experiments, our hyperparameters will be:\")\n",
        "print(\"    + n_hid_units={}\".format(n_hid_units))\n",
        "print(\"    + n_episodes={}\".format(n_episodes))\n",
        "print(\"    + learning_rate={}\".format(learning_rate))\n",
        "print(\"    + update_freq={}\".format(update_freq))"
      ],
      "metadata": {
        "id": "MYUNDBhQ-Wsh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_graphs_qwc, losses = TB_training(Gc, n_terms, n_hid_units, n_episodes, learning_rate, update_freq, seed, n_q)"
      ],
      "metadata": {
        "id": "4QhCd4FE-n2H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Result analysis functions"
      ],
      "metadata": {
        "id": "nxepotco_h-C"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_loss_curve(figure, losses_A, title=\"\"):\n",
        "    filename = f\"{figure}_loss.svg\"\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.plot(losses_A, color=\"black\")\n",
        "    plt.yscale(\"log\")\n",
        "    plt.xlabel(\"Step\")\n",
        "    plt.ylabel(\"Loss\")\n",
        "    plt.savefig(filename, format='svg', dpi=600)\n",
        "\n",
        "def check_sampled_graphs_vqe_plot(figure, sampled_graphs):\n",
        "    filename = f\"{figure}_graphs.png\"\n",
        "\n",
        "    \"\"\"Check sampled graphs with no duplicates based on vqe/number of shots and graphs them\"\"\"\n",
        "    fig, ax = plt.subplots(4, 4, figsize=(20, 20))\n",
        "    n_plot = 16  # 4 x 4\n",
        "\n",
        "    print('Proportion of valid graphs:{}, ideal=1'.format(\n",
        "        sum([color_reward(i) > 0 for i in sampled_graphs]) / len(sampled_graphs)\n",
        "    ))\n",
        "\n",
        "    # Sort graphs by reward, but filter out those with duplicate color dictionaries\n",
        "    unique_graphs = []\n",
        "    seen_color_dicts = set()\n",
        "\n",
        "    for graph in sorted(sampled_graphs, key=lambda i: shots_estimator(i), reverse=False):\n",
        "        color_dict = frozenset(nx.get_node_attributes(graph, \"color\").items())\n",
        "        if color_dict not in seen_color_dicts:\n",
        "            seen_color_dicts.add(color_dict)\n",
        "            unique_graphs.append(graph)\n",
        "\n",
        "    print('Number of unique graphs ={}'.format(len(unique_graphs)))\n",
        "    print('Number of shots for the best {} graphs'.format(n_plot))\n",
        "\n",
        "    for i in range(n_plot):\n",
        "      print('Number of shots={}, max color {} and reward {}'.format(shots_estimator(unique_graphs[i]), max_color(unique_graphs[i]),vqe_reward(unique_graphs[i])))\n",
        "      plt.sca(ax[i//4, i%4])\n",
        "      plot_graph_wcolor(unique_graphs[i])\n",
        "\n",
        "    plt.savefig(filename, format='png')\n",
        "\n",
        "def plot_graph_wcolor(graph):\n",
        "    #print([data['color'] for _, data in graph.nodes(data=True)])\n",
        "    colors_dict = nx.get_node_attributes(graph, \"color\")\n",
        "    vector = list(colors_dict.values())\n",
        "    pos = nx.circular_layout(graph)\n",
        "    #pos = nx.kamada_kawai_layout(graph)\n",
        "    options = {\n",
        "    \"pos\": pos,\n",
        "    \"node_color\": vector,\n",
        "    \"node_size\": 300,\n",
        "    \"edge_color\": \"gray\",\n",
        "    \"alpha\": 0.9,\n",
        "    \"width\": 6,\n",
        "    \"labels\": {n: n for n in graph}\n",
        "    }\n",
        "    #For H2 add labels and circ layout. node_size=300, width=6\n",
        "    nx.draw(graph, cmap=plt.cm.rainbow, **options)\n",
        "    shots = shots_estimator(graph)\n",
        "    color = max_color(graph)\n",
        "    plt.text(0.01, 0.98, f'Shots:',\n",
        "             horizontalalignment='left', verticalalignment='top',\n",
        "             transform=plt.gca().transAxes, fontsize=12, color='black')\n",
        "    plt.text(0.01, 0.92, f'{shots}',\n",
        "             horizontalalignment='left', verticalalignment='top',\n",
        "             transform=plt.gca().transAxes, fontsize=12, color='black')\n",
        "    plt.text(0.01, 0.85, f'Colors:',\n",
        "             horizontalalignment='left', verticalalignment='top',\n",
        "             transform=plt.gca().transAxes, fontsize=12, color='black')\n",
        "    plt.text(0.01, 0.8, f'{color}',\n",
        "             horizontalalignment='left', verticalalignment='top',\n",
        "             transform=plt.gca().transAxes, fontsize=12, color='black')\n",
        "\n",
        "def histogram_all(figure, sampled_graphs):\n",
        "    filename = f\"{figure}_histo_all.svg\"\n",
        "    n_shots = [shots_estimator(i)*1E-6 for i in sampled_graphs]\n",
        "    color = [max_color(i) for i in sampled_graphs]\n",
        "    print('Minimum number of groups found {}'.format(min(color)))\n",
        "    x_bins = np.arange(min(color) - 0.5, max(color)  + 0.5, 1)  # Center the bars on integer ticks\n",
        "    y_bins = np.linspace(min(n_shots), max(n_shots), 50)  # You can adjust the number of bins\n",
        "# Create 2D histogram\n",
        "    plt.figure(figsize=(10,5))\n",
        "    plt.hist2d(color, n_shots, bins=[x_bins, y_bins])\n",
        "# Add color bar for intensity reference\n",
        "    plt.colorbar(label='Sampled graphs')\n",
        "# Label axes\n",
        "    plt.xticks(np.arange(min(color),max(color),step=1,dtype=np.int32)) #Requests only integers on x\n",
        "    plt.xlabel('Max Color')\n",
        "    plt.ylabel(r'$M_{est}\\  \\ [\\times 10^{6}]$')\n",
        "    plt.savefig(filename, format='svg', dpi=600)\n"
      ],
      "metadata": {
        "id": "0HXHJDl_-37B"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_sampled_graphs_vqe_plot(fig_name, sampled_graphs_qwc)"
      ],
      "metadata": {
        "id": "qlIlzAPgAbTs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(fig_name, losses, title=\"Loss over Training Iterations\")"
      ],
      "metadata": {
        "id": "bxMRPhaPAlaU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_all(fig_name,sampled_graphs_qwc)"
      ],
      "metadata": {
        "id": "opgjzFmpBBhv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we repeat for **Fully Commuting**"
      ],
      "metadata": {
        "id": "LoJIB0X6FSSZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "binary_H = BinaryHamiltonian.init_from_qubit_hamiltonian(H)\n",
        "terms=get_terms(binary_H)\n",
        "CompMatrix=FC_CompMatrix(terms)\n",
        "#CompMatrix=QWC_CompMatrix(terms)\n",
        "Gc=obj_to_comp_graph(terms, CompMatrix)\n",
        "n_terms=nx.number_of_nodes(Gc)\n",
        "print(\"Number of terms in the graph: {}\".format(n_terms))\n",
        "n_hid_units = 512\n",
        "n_episodes = 5000\n",
        "learning_rate = 5e-3 #5e-4 1e-3 try on your own\n",
        "update_freq = 10\n",
        "seed = 99\n",
        "fig_name = \"Test\"\n",
        "\n",
        "print(\"For all experiments, our hyperparameters will be:\")\n",
        "print(\"    + n_hid_units={}\".format(n_hid_units))\n",
        "print(\"    + n_episodes={}\".format(n_episodes))\n",
        "print(\"    + learning_rate={}\".format(learning_rate))\n",
        "print(\"    + update_freq={}\".format(update_freq))\n",
        "\n",
        "pos = nx.circular_layout(Gc)\n",
        "options = {\n",
        "    \"pos\": pos,\n",
        "    \"node_size\": 1200,\n",
        "    \"edge_color\": \"gray\",\n",
        "    \"alpha\": 1,\n",
        "    \"width\": 4,\n",
        "    \"labels\": {n: n for n in Gc}\n",
        "}\n",
        "plt.figure(1,figsize=(5,5))\n",
        "nx.draw_networkx(Gc,**options)"
      ],
      "metadata": {
        "id": "kag7vgqkFVlv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sampled_graphs, losses = TB_training(Gc, n_terms, n_hid_units, n_episodes, learning_rate, update_freq, seed, n_q)"
      ],
      "metadata": {
        "id": "fQYSv871FlC-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "check_sampled_graphs_vqe_plot(fig_name, sampled_graphs)"
      ],
      "metadata": {
        "id": "erVwD8z3FxdA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plot_loss_curve(fig_name, losses, title=\"Loss over Training Iterations\")"
      ],
      "metadata": {
        "id": "KNYa-DrZFyFV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "histogram_all(fig_name,sampled_graphs)"
      ],
      "metadata": {
        "id": "Fm49tkrCpfVw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Summary of Results\n",
        "| Methods      | Num. of Measurments |\n",
        "| ----------- | ----------- |\n",
        "| No grouping     | 1414072      |\n",
        "|   |       |\n",
        "| RLF/FC | 163539 |\n",
        "|  RLF/QWC | 329884 |\n",
        "| GFlowNets/FC | 113270 |\n",
        "| GFlowNets/QWC | 158446 |\n",
        "\n"
      ],
      "metadata": {
        "id": "bPTQ5yThMehx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Lets see how to measure the energy from our graphs"
      ],
      "metadata": {
        "id": "ewrZ4F1I3o1b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "unique_graphs = []\n",
        "seen_color_dicts = set()\n",
        "\n",
        "for graph in sorted(sampled_graphs_qwc, key=lambda i: shots_estimator(i), reverse=False):\n",
        "    color_dict = frozenset(nx.get_node_attributes(graph, \"color\").items())\n",
        "    if color_dict not in seen_color_dicts:\n",
        "        seen_color_dicts.add(color_dict)\n",
        "        unique_graphs.append(graph)\n",
        "\n",
        "plot_graph_wcolor(unique_graphs[0])"
      ],
      "metadata": {
        "id": "sJIKneP_2FYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(vqe_reward(unique_graphs[0]))\n",
        "h_by_color = extract_hamiltonian_by_color(unique_graphs[0])\n",
        "groups = generate_groups(h_by_color)\n",
        "imported_operators = convert_operators(groups)\n",
        "\n",
        "obs_groupings = []\n",
        "c_groupings = []\n",
        "H_group = []\n",
        "for i in range(len(imported_operators)):\n",
        "    coeffs, obs = imported_operators[i].terms()\n",
        "    obs_groupings.append(obs)\n",
        "    c_groupings.append(coeffs)\n",
        "    H_group.append(qml.Hamiltonian(coeffs=coeffs, observables=obs))\n",
        "\n",
        "rotations, circuits = qml.pauli.diagonalize_qwc_groupings(obs_groupings)\n",
        "print(rotations)\n",
        "print(circuits)\n",
        "print(\"Number of circuits to run: {}\".format(len(circuits)))"
      ],
      "metadata": {
        "id": "XllfAOEEBGy6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dev = qml.device(\"lightning.qubit\", wires=4)\n",
        "num_qubits = 4\n",
        "\n",
        "@qml.qnode(dev, interface=\"jax\")\n",
        "def circuit(params, group=None, **kwargs):\n",
        "    for w in range(num_qubits):\n",
        "        qml.Hadamard(wires=w)\n",
        "        qml.RY(params[w], wires=w)\n",
        "    for w in dev.wires[:-1]:\n",
        "        qml.CNOT(wires=[w, w + 1])\n",
        "    for w in dev.wires:\n",
        "        qml.RZ(params[w + num_qubits], wires=w)\n",
        "    return [qml.expval(o) for o in group]\n",
        "\n",
        "params = np.random.randn(2 * num_qubits)\n",
        "result = [jnp.array(circuit(params, group=g)) for g in H_group]\n",
        "\n",
        "print(\"Term expectation values:\")\n",
        "for group, expvals in enumerate(result):\n",
        "    print(f\"Group {group} expectation values:\", expvals)\n",
        "\n",
        "# Since all the coefficients of the Hamiltonian are unity,\n",
        "# we can simply sum the expectation values.\n",
        "print(\"<H> = \", jnp.sum(jnp.hstack(result)))"
      ],
      "metadata": {
        "id": "a55Lc4v-wuSq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Homework: LiH\n",
        "Due to time constrains, we can not do the same study for LiH. However, you can use the following code to generate the Hamiltonian for LiH.\n",
        "\n",
        "\n",
        "```Python\n",
        "def LiH():\n",
        "    '''\n",
        "    LiH hamiltonian.\n",
        "    '''\n",
        "    trafo = \"JordanWigner\"\n",
        "    mol = tq.chemistry.Molecule(\n",
        "                            geometry=\"Li 0.0 0.0 0.0 \\n H 0.0 0.0 1.\",\n",
        "                            basis_set=\"sto3g\",\n",
        "                            transformation=trafo,\n",
        "                            backend='pyscf'\n",
        "                                 )\n",
        "    H = mol.make_hamiltonian()\n",
        "    Hq = H.to_openfermion()\n",
        "    Hferm = reverse_jordan_wigner(Hq)\n",
        "    # Minus 1 since it always contain a constant term that we don't need to measure.\n",
        "    return mol, H, Hferm, len(Hq.terms) - 1, Hq\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "F8MLAxxCL6te"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "pY-Nu85sL-_2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}